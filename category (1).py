# -*- coding: utf-8 -*-
"""Category.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YxR8oaC8YPlaYrzsX2fqShqVsUX5A-iF
"""

# --- 1. INSTALL LIBRARIES ---
!pip install transformers[torch] datasets accelerate -q
# Run this cell first to update to the latest versions
!pip install --upgrade transformers datasets accelerate

# --- 2. IMPORT LIBRARIES ---
import pandas as pd
import torch
import numpy as np
from sklearn.model_selection import train_test_split
from datasets import Dataset, DatasetDict
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    Trainer,
    TrainingArguments,
    pipeline
)

# --- 3. LOAD YOUR DATASET (Corrected) ---
file_path = '/content/Book1.xlsx'
try:
    # --- FIX IS HERE ---
    # We explicitly tell pandas to read only the first 3 columns (indexed 0, 1, 2)
    # and assign our desired names, skipping the potentially messy header.
    df = pd.read_excel(
        file_path,
        usecols=[0, 1, 2],
        names=['Category', 'SubCategory', 'FeedbackText'],
        header=0  # Use this if your file has a header row to skip
    )
    # Remove any completely empty rows that might have been loaded
    df.dropna(inplace=True)

    print("Dataset loaded and cleaned successfully!")

except FileNotFoundError:
    print(f"Error: Make sure the file '{file_path}' is uploaded.")
    df = pd.DataFrame({
        'Category': ['Academic & Curriculum', 'Infrastructure & Facilities'],
        'SubCategory': ['Teaching Quality', 'Classroom Conditions'],
        'FeedbackText': ['The professor just reads from the slides', 'The benches are broken']
    })

# --- 4. PREPARE LABELS ---
# (The rest of the code from here is the same)
unique_categories = df['Category'].unique().tolist()
unique_subcategories = df['SubCategory'].unique().tolist()

cat_label2id = {label: i for i, label in enumerate(unique_categories)}
cat_id2label = {i: label for i, label in enumerate(unique_categories)}

subcat_label2id = {label: i for i, label in enumerate(unique_subcategories)}
subcat_id2label = {i: label for i, label in enumerate(unique_subcategories)}

df['cat_labels'] = df['Category'].map(cat_label2id)
df['subcat_labels'] = df['SubCategory'].map(subcat_label2id)

print("\nData prepared with numerical labels:")
print(df.head())

# --- 5. CREATE THE DATASET FOR HUGGING FACE ---
dataset = Dataset.from_pandas(df)

# --- 1. SET UP TOKENIZER AND MODEL ---
# This part remains the same.
model_name = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
cat_model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=len(unique_categories),
    id2label=cat_id2label,
    label2id=cat_label2id
)

# --- 2. TOKENIZE THE DATASET ---
# This part also remains the same.
def tokenize(batch):
    return tokenizer(batch['FeedbackText'], padding=True, truncation=True, max_length=512)

dataset_encoded = dataset.map(tokenize, batched=True)

# --- 3. SPLIT DATA INTO TRAINING AND TEST SETS ---
train_test_split_dataset = dataset_encoded.train_test_split(test_size=0.2, seed=42)

# --- 4. PREPARE DATASET FOR THE TRAINER ---
# This section cleans the dataset to prevent errors.
train_dataset_cat = train_test_split_dataset['train']
eval_dataset_cat = train_test_split_dataset['test']

# Rename the label column to 'labels', which the Trainer expects.
train_dataset_cat = train_dataset_cat.rename_column("cat_labels", "labels")
eval_dataset_cat = eval_dataset_cat.rename_column("cat_labels", "labels")

# Remove all columns the model doesn't need.
columns_to_remove = ['Category', 'SubCategory', 'FeedbackText', 'subcat_labels']
train_dataset_cat = train_dataset_cat.remove_columns(columns_to_remove)
eval_dataset_cat = eval_dataset_cat.remove_columns(columns_to_remove)

# --- 5. DEFINE TRAINING ARGUMENTS ---
# This section now uses the modern arguments that work with the updated library.
training_args_cat = TrainingArguments(
    output_dir="./models/category_classifier",
    num_train_epochs=3,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    weight_decay=0.01,
    eval_strategy="epoch",  # Correct argument for new versions
    logging_strategy="epoch",     # Correct argument for new versions
    save_strategy="epoch",        # Correct argument for new versions
    load_best_model_at_end=True,
    report_to="none", # Disable W&B logging
)

# --- 6. CREATE AND RUN THE TRAINER ---
# This part remains the same.
trainer_cat = Trainer(
    model=cat_model,
    args=training_args_cat,
    train_dataset=train_dataset_cat,
    eval_dataset=eval_dataset_cat,
    tokenizer=tokenizer
)

print("\n--- Starting to Fine-Tune the Category Classifier ---")
trainer_cat.train()
print("--- Category Classifier Fine-Tuning Complete! ---")

# --- 7. SAVE THE MODEL ---
trainer_cat.save_model("./models/best_category_model")

# --- 1. SET UP THE SUBCATEGORY MODEL ---
# We load a fresh model to train specifically for subcategories.
subcat_model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=len(unique_subcategories), # The number of subcategories
    id2label=subcat_id2label,
    label2id=subcat_label2id
)

# --- 2. PREPARE DATASET FOR THE TRAINER ---
# This section cleans the dataset to prevent errors.
train_dataset_subcat = train_test_split_dataset['train']
eval_dataset_subcat = train_test_split_dataset['test']

# Rename the correct label column to 'labels', which the Trainer expects.
train_dataset_subcat = train_dataset_subcat.rename_column("subcat_labels", "labels")
eval_dataset_subcat = eval_dataset_subcat.rename_column("subcat_labels", "labels")

# Remove all columns the model doesn't need for this specific task.
columns_to_remove = ['Category', 'SubCategory', 'FeedbackText', 'cat_labels']
train_dataset_subcat = train_dataset_subcat.remove_columns(columns_to_remove)
eval_dataset_subcat = eval_dataset_subcat.remove_columns(columns_to_remove)

# --- 3. DEFINE TRAINING ARGUMENTS ---
# This uses the modern arguments compatible with the updated libraries.
training_args_subcat = TrainingArguments(
    output_dir="./models/subcategory_classifier",
    num_train_epochs=3,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    weight_decay=0.01,
    eval_strategy="epoch",
    logging_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",
    report_to="none" # Disable W&B logging
)

# --- 4. DEFINE METRICS (for subcategory model) ---
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

def compute_metrics_subcat(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted', zero_division=0)
    acc = accuracy_score(labels, preds)
    return {
        'accuracy': acc,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }

# --- 5. CREATE AND RUN THE TRAINER ---
# The Trainer handles the entire fine-tuning process.
trainer_subcat = Trainer(
    model=subcat_model,
    args=training_args_subcat,
    train_dataset=train_dataset_subcat,
    eval_dataset=eval_dataset_subcat,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics_subcat, # Add compute_metrics here
)

print("\n--- Starting to Fine-Tune the SubCategory Classifier ---")
trainer_subcat.train()
print("--- SubCategory Classifier Fine-Tuning Complete! ---")

# --- 6. SAVE THE MODEL ---
trainer_subcat.save_model("./models/best_subcategory_model")

# Block 4 (Corrected): Build the All-in-One Prediction Pipeline

import torch
from transformers import pipeline
import json

class FeedbackAnalyzer:
    def __init__(self, category_model_path, subcategory_model_path, base_tokenizer_name):
        """
        Initializes the analyzer by loading all three models into memory.
        """
        device = 0 if torch.cuda.is_available() else -1

        # 1. Load the pre-trained Sentiment Analysis pipeline
        print("Loading sentiment analysis model...")
        self.sentiment_pipeline = pipeline(
            "sentiment-analysis",
            model="cardiffnlp/twitter-roberta-base-sentiment-latest",
            device=device
        )

        # 2. Load your fine-tuned Category Classifier
        print("Loading fine-tuned category model...")
        self.category_pipeline = pipeline(
            "text-classification",
            model=category_model_path,
            tokenizer=base_tokenizer_name,
            device=device
        )

        # 3. Load your fine-tuned SubCategory Classifier
        print("Loading fine-tuned subcategory model...")
        self.subcategory_pipeline = pipeline(
            "text-classification",
            model=subcategory_model_path,
            tokenizer=base_tokenizer_name,
            device=device
        )
        print("\nâœ… Feedback Analyzer is ready to use!")

    def analyze(self, feedback_text):
        """
        Analyzes a piece of feedback text to get a complete report.
        """
        sentiment_result = self.sentiment_pipeline(feedback_text)[0]
        category_result = self.category_pipeline(feedback_text)[0]
        subcategory_result = self.subcategory_pipeline(feedback_text)[0]

        # --- FIX IS HERE ---
        # The 'cardiffnlp' model directly returns 'positive', 'negative', or 'neutral'.
        # We no longer need the old sentiment_map. We just capitalize the result.
        sentiment_label = sentiment_result['label'].capitalize()

        # Structure the final output into a clean dictionary
        analysis = {
            "sentiment": sentiment_label,
            "confidence_score": f"{sentiment_result['score']:.4f}",
            "category": category_result['label'],
            "subcategory": subcategory_result['label']
        }

        return analysis

# --- HOW TO USE YOUR FINE-TUNED MODELS ---

# 1. Create an instance of the analyzer.
analyzer = FeedbackAnalyzer(
    category_model_path="./models/best_category_model",
    subcategory_model_path="./models/best_subcategory_model",
    base_tokenizer_name="distilbert-base-uncased"
)

# 2. Test with new feedback.
new_feedback = "The professor is very arrogant and unprofessional."

# 3. Call the .analyze() method to get predictions.
prediction = analyzer.analyze(new_feedback)

# 4. Print the results.
print("\n--- Analysis Result ---")
print(json.dumps(prediction, indent=2))

# Ensure the 'analyzer' object from the previous block is already created.

# --- Get Manual Feedback from the User ---
# This line will create a text box for you to type in.
manual_feedback = input("Please enter the student feedback you want to analyze: ")

# Check if the user entered any text before analyzing
if manual_feedback and manual_feedback.strip():
    # Run the analysis on the feedback you entered
    prediction = analyzer.analyze(manual_feedback)

    # Print the results in a nicely formatted way
    print("\n--- Analysis Result ---")
    import json
    print(json.dumps(prediction, indent=2))
else:
    print("\nNo feedback was entered. Please run the cell again and provide some text.")

# --- Block 5: Create the Backend API Server ---

# 1. Install Flask and pyngrok
!pip install flask pyngrok flask-cors -q

from flask import Flask, request, jsonify
from pyngrok import ngrok
from flask_cors import CORS
import json

# This code assumes the 'analyzer' object from Block 4 is already created and in memory.

# 2. Set up the Flask App
app = Flask(__name__)
CORS(app) # Allows your local app to talk to this server

# 3. Define the API endpoint that will receive feedback
@app.route('/analyze', methods=['POST'])
def analyze_feedback():
    try:
        data = request.get_json()
        if 'feedback_text' not in data or not data['feedback_text'].strip():
            return jsonify({"error": "Missing 'feedback_text' key or text is empty"}), 400

        feedback = data['feedback_text']

        # Use the powerful 'analyzer' object you already trained!
        prediction = analyzer.analyze(feedback)

        return jsonify(prediction)

    except Exception as e:
        return jsonify({"error": str(e)}), 500

# 4. Expose the Flask app to the internet using ngrok
public_url = ngrok.connect(5000)
print("âœ… Your Colab backend is live!")
print(f"ðŸ”— Public URL: {public_url}")
print("COPY THIS URL and paste it into your 'local_app.py' file.")

# 5. Run the app. This cell will keep running.
app.run(port=5000)

